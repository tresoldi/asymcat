{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASymCat: Comprehensive Examples with Mathematical Analysis\n",
    "\n",
    "This notebook demonstrates ASymCat's capabilities with real datasets, mathematical explanations, and comprehensive visualizations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Mathematical Foundations](#Mathematical-Foundations)\n",
    "2. [Species Co-occurrence: Galápagos Finches](#Species-Co-occurrence)\n",
    "3. [Linguistic Analysis: CMU Pronunciation Dictionary](#Linguistic-Analysis)\n",
    "4. [Classification Analysis: Mushroom Edibility](#Classification-Analysis)\n",
    "5. [Comparative Analysis: Pokémon Type Relationships](#Comparative-Analysis)\n",
    "6. [Advanced: Smoothing Effects](#Advanced-Smoothing)\n",
    "7. [Interpretation Guidelines](#Interpretation-Guidelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import asymcat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"ASymCat version:\", asymcat.__version__ if hasattr(asymcat, '__version__') else \"Latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations\n",
    "\n",
    "### Asymmetric vs Symmetric Association\n",
    "\n",
    "**Symmetric measures** treat relationships as bidirectional:\n",
    "- χ² (Chi-square): Tests independence\n",
    "- PMI (Pointwise Mutual Information): Measures information overlap\n",
    "- Jaccard Index: Set overlap similarity\n",
    "\n",
    "**Asymmetric measures** capture directional dependencies:\n",
    "- MLE: Conditional probability P(Y|X) ≠ P(X|Y)\n",
    "- Theil's U: Proportional uncertainty reduction U(Y|X) ≠ U(X|Y)\n",
    "- Lambda: Prediction error reduction λ(Y|X) ≠ λ(X|Y)\n",
    "\n",
    "### Simple Mathematical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple asymmetric dataset\n",
    "simple_data = [\n",
    "    ('A', 'c'), ('A', 'c'), ('A', 'd'),  # A can lead to c or d\n",
    "    ('B', 'g'), ('B', 'g'), ('B', 'f'),  # B can lead to g or f\n",
    "]\n",
    "\n",
    "cooccs = asymcat.collect_cooccs([simple_data])\n",
    "scorer = asymcat.scorer.CatScorer(cooccs)\n",
    "\n",
    "# Compute measures\n",
    "mle_scores = scorer.mle()\n",
    "pmi_scores = scorer.pmi()\n",
    "theil_scores = scorer.theil_u()\n",
    "\n",
    "print(\"Mathematical Example: Perfect Asymmetry\")\n",
    "print(\"=====================================\")\n",
    "print(\"Data: A→{c,c,d}, B→{g,g,f}\")\n",
    "print()\n",
    "\n",
    "for pair in [('A', 'c'), ('B', 'g')]:\n",
    "    if pair in mle_scores:\n",
    "        xy, yx = mle_scores[pair]\n",
    "        pmi_val = pmi_scores[pair][0]  # PMI is symmetric\n",
    "        u_xy, u_yx = theil_scores[pair]\n",
    "        \n",
    "        print(f\"Pair {pair}:\")\n",
    "        print(f\"  MLE: P({pair[1]}|{pair[0]}) = {xy:.3f}, P({pair[0]}|{pair[1]}) = {yx:.3f}\")\n",
    "        print(f\"  PMI: {pmi_val:.3f} (symmetric)\")\n",
    "        print(f\"  Theil U: U({pair[1]}|{pair[0]}) = {u_xy:.3f}, U({pair[0]}|{pair[1]}) = {u_yx:.3f}\")\n",
    "        print()\n",
    "\n",
    "# Visualize the simple example\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Extract data for plotting\n",
    "pairs = list(mle_scores.keys())\n",
    "mle_xy = [mle_scores[p][0] for p in pairs]\n",
    "mle_yx = [mle_scores[p][1] for p in pairs]\n",
    "pair_labels = [f\"({p[0]},{p[1]})\" for p in pairs]\n",
    "\n",
    "# MLE comparison\n",
    "x = np.arange(len(pairs))\n",
    "width = 0.35\n",
    "axes[0].bar(x - width/2, mle_xy, width, label='X→Y', alpha=0.8)\n",
    "axes[0].bar(x + width/2, mle_yx, width, label='Y→X', alpha=0.8)\n",
    "axes[0].set_xlabel('Pair')\n",
    "axes[0].set_ylabel('MLE Score')\n",
    "axes[0].set_title('MLE: Conditional Probabilities')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(pair_labels, rotation=45)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PMI (symmetric)\n",
    "pmi_vals = [pmi_scores[p][0] for p in pairs]\n",
    "axes[1].bar(x, pmi_vals, alpha=0.8, color='green')\n",
    "axes[1].set_xlabel('Pair')\n",
    "axes[1].set_ylabel('PMI Score')\n",
    "axes[1].set_title('PMI: Symmetric Information')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(pair_labels, rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Theil's U comparison\n",
    "theil_xy = [theil_scores[p][0] for p in pairs]\n",
    "theil_yx = [theil_scores[p][1] for p in pairs]\n",
    "axes[2].bar(x - width/2, theil_xy, width, label='X→Y', alpha=0.8)\n",
    "axes[2].bar(x + width/2, theil_yx, width, label='Y→X', alpha=0.8)\n",
    "axes[2].set_xlabel('Pair')\n",
    "axes[2].set_ylabel('Theil U Score')\n",
    "axes[2].set_title('Theil U: Uncertainty Reduction')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(pair_labels, rotation=45)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species Co-occurrence: Galápagos Finches\n",
    "\n",
    "The Galápagos dataset contains presence-absence data for 13 finch species across 17 islands. This is a classic example of **species co-occurrence analysis** where we want to understand:\n",
    "\n",
    "1. Which species **co-occur** more often than expected by chance?\n",
    "2. Are there **asymmetric relationships** (species A predicts species B, but not vice versa)?\n",
    "3. What are the **ecological implications** of these patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Galápagos finch data\n",
    "galapagos_data = asymcat.read_pa_matrix(\"../resources/galapagos.tsv\")\n",
    "galapagos_cooccs = asymcat.collect_cooccs(galapagos_data)\n",
    "\n",
    "print(f\"Galápagos Dataset: {len(galapagos_cooccs)} co-occurrences\")\n",
    "print(f\"Species pairs analyzed: {len(galapagos_cooccs)}\")\n",
    "\n",
    "# Create scorer with Laplace smoothing for better estimates with sparse data\n",
    "galapagos_scorer = asymcat.scorer.CatScorer(galapagos_cooccs, \n",
    "                                          smoothing_method='laplace')\n",
    "\n",
    "# Compute multiple measures\n",
    "measures = {\n",
    "    'MLE': galapagos_scorer.mle(),\n",
    "    'PMI': galapagos_scorer.pmi(),\n",
    "    'Jaccard': galapagos_scorer.jaccard_index(),\n",
    "    'Theil_U': galapagos_scorer.theil_u()\n",
    "}\n",
    "\n",
    "# Display raw data structure\n",
    "df = pd.read_csv(\"../resources/galapagos.tsv\", sep='\\t', index_col=0)\n",
    "print(f\"\\nData shape: {df.shape} (islands × species)\")\n",
    "print(f\"Species: {list(df.columns)[:5]}...\")  # Show first 5 species\n",
    "print(f\"Islands: {list(df.index)[:5]}...\")   # Show first 5 islands\n",
    "\n",
    "# Show presence/absence matrix\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(df.T, cmap='RdYlBu_r', cbar_kws={'label': 'Presence/Absence'}, \n",
    "            xticklabels=True, yticklabels=True)\n",
    "plt.title('Galápagos Finch Species Distribution Across Islands', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Islands')\n",
    "plt.ylabel('Species')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze strongest associations\n",
    "def analyze_top_associations(scores, measure_name, n_top=10):\n",
    "    \"\"\"Analyze top associations for a given measure.\"\"\"\n",
    "    print(f\"\\n=== {measure_name} Analysis ===\")\n",
    "    \n",
    "    # Get all scores and sort by strength\n",
    "    all_scores = []\n",
    "    for pair, (xy, yx) in scores.items():\n",
    "        all_scores.append((pair, xy, yx, max(xy, yx)))\n",
    "    \n",
    "    # Sort by maximum score in either direction\n",
    "    all_scores.sort(key=lambda x: x[3], reverse=True)\n",
    "    \n",
    "    print(f\"Top {n_top} associations:\")\n",
    "    for i, (pair, xy, yx, max_score) in enumerate(all_scores[:n_top]):\n",
    "        direction = \"→\" if xy > yx else \"←\"\n",
    "        print(f\"{i+1:2d}. {pair[0]} {direction} {pair[1]}: \"\n",
    "              f\"{xy:.3f} (X→Y), {yx:.3f} (Y→X)\")\n",
    "    \n",
    "    return all_scores[:n_top]\n",
    "\n",
    "# Analyze each measure\n",
    "top_mle = analyze_top_associations(measures['MLE'], 'MLE (Conditional Probability)')\n",
    "top_jaccard = analyze_top_associations(measures['Jaccard'], 'Jaccard Index')\n",
    "top_theil = analyze_top_associations(measures['Theil_U'], 'Theil Uncertainty Coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize association networks\n",
    "def create_association_heatmap(scores, title, threshold=0.5):\n",
    "    \"\"\"Create heatmap of associations above threshold.\"\"\"\n",
    "    \n",
    "    # Get all unique species\n",
    "    all_species = set()\n",
    "    for pair in scores.keys():\n",
    "        all_species.update(pair)\n",
    "    species_list = sorted(all_species)\n",
    "    \n",
    "    # Create matrices\n",
    "    n = len(species_list)\n",
    "    xy_matrix = np.zeros((n, n))\n",
    "    yx_matrix = np.zeros((n, n))\n",
    "    \n",
    "    species_to_idx = {sp: i for i, sp in enumerate(species_list)}\n",
    "    \n",
    "    for (sp1, sp2), (xy, yx) in scores.items():\n",
    "        i, j = species_to_idx[sp1], species_to_idx[sp2]\n",
    "        xy_matrix[i, j] = xy\n",
    "        yx_matrix[j, i] = yx  # Note the transpose for Y→X\n",
    "    \n",
    "    # Create asymmetric matrix (show strongest direction)\n",
    "    asym_matrix = np.maximum(xy_matrix, yx_matrix.T)\n",
    "    \n",
    "    # Mask weak associations\n",
    "    asym_matrix = np.where(asym_matrix > threshold, asym_matrix, 0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Shorten species names for display\n",
    "    short_names = [name.split('.')[0] if '.' in name else name[:10] \n",
    "                   for name in species_list]\n",
    "    \n",
    "    sns.heatmap(asym_matrix, \n",
    "                xticklabels=short_names,\n",
    "                yticklabels=short_names,\n",
    "                annot=True, fmt='.2f', \n",
    "                cmap='RdYlBu_r', center=threshold,\n",
    "                square=True,\n",
    "                linewidths=0.5)\n",
    "    \n",
    "    plt.title(f'{title}\\n(Associations > {threshold})', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Species B')\n",
    "    plt.ylabel('Species A')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return asym_matrix, species_list\n",
    "\n",
    "# Create heatmaps for different measures\n",
    "mle_matrix, species = create_association_heatmap(measures['MLE'], \n",
    "                                                'Species Co-occurrence: MLE Associations', \n",
    "                                                threshold=0.3)\n",
    "\n",
    "jaccard_matrix, _ = create_association_heatmap(measures['Jaccard'], \n",
    "                                              'Species Co-occurrence: Jaccard Similarity', \n",
    "                                              threshold=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecological Interpretation\n",
    "\n",
    "The Galápagos analysis reveals several important ecological patterns:\n",
    "\n",
    "1. **Habitat Specialization**: Some species co-occur frequently, suggesting shared habitat preferences\n",
    "2. **Island Biogeography**: Larger islands (like Isabela) support more species diversity\n",
    "3. **Competitive Exclusion**: Some species pairs never co-occur, possibly due to competition\n",
    "4. **Asymmetric Dependencies**: Species A may predict species B's presence better than vice versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistic Analysis: CMU Pronunciation Dictionary\n",
    "\n",
    "Analyzing orthography-to-phonetics mappings reveals **systematic patterns** in English pronunciation:\n",
    "\n",
    "- **Grapheme-Phoneme Correspondence**: How letters map to sounds\n",
    "- **Phonotactic Patterns**: Which sound combinations are common\n",
    "- **Asymmetric Relationships**: Orthography may predict phonetics better than vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CMU dictionary data\n",
    "cmu_data = asymcat.read_sequences(\"../resources/cmudict.sample1000.tsv\")\n",
    "cmu_cooccs = asymcat.collect_cooccs(cmu_data)\n",
    "\n",
    "print(f\"CMU Dictionary Sample: {len(cmu_data)} words\")\n",
    "print(f\"Co-occurrences: {len(cmu_cooccs)} orthography-phoneme pairs\")\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample orthography-phoneme alignments:\")\n",
    "for i in range(5):\n",
    "    ortho, phon = cmu_data[i]\n",
    "    print(f\"'{' '.join(ortho)}' → /{' '.join(phon)}/\")\n",
    "\n",
    "# Create scorer with different smoothing methods\n",
    "cmu_scorer_mle = asymcat.scorer.CatScorer(cmu_cooccs, smoothing_method='mle')\n",
    "cmu_scorer_smooth = asymcat.scorer.CatScorer(cmu_cooccs, smoothing_method='laplace')\n",
    "\n",
    "# Compute measures\n",
    "cmu_mle = cmu_scorer_mle.mle()\n",
    "cmu_mle_smooth = cmu_scorer_smooth.mle()\n",
    "cmu_pmi = cmu_scorer_mle.pmi()\n",
    "cmu_pmi_smooth = cmu_scorer_smooth.pmi_smoothed()\n",
    "\n",
    "print(f\"\\nMeasures computed for {len(cmu_mle)} orthography-phoneme pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze orthography-phoneme correspondences\n",
    "def analyze_grapheme_phoneme(scores, title, n_top=15):\n",
    "    \"\"\"Analyze strongest grapheme-phoneme correspondences.\"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    \n",
    "    # Separate orthography→phoneme and phoneme→orthography\n",
    "    ortho_to_phon = []\n",
    "    phon_to_ortho = []\n",
    "    \n",
    "    for (x, y), (xy, yx) in scores.items():\n",
    "        # Assume orthography are letters, phonemes have special characters\n",
    "        if len(x) == 1 and x.isalpha():  # x is likely orthography\n",
    "            ortho_to_phon.append((x, y, xy))\n",
    "            phon_to_ortho.append((y, x, yx))\n",
    "        elif len(y) == 1 and y.isalpha():  # y is likely orthography\n",
    "            ortho_to_phon.append((y, x, yx))\n",
    "            phon_to_ortho.append((x, y, xy))\n",
    "    \n",
    "    # Sort and display top correspondences\n",
    "    ortho_to_phon.sort(key=lambda x: x[2], reverse=True)\n",
    "    phon_to_ortho.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop {n_top//2} Orthography → Phoneme:\")\n",
    "    for i, (ortho, phon, score) in enumerate(ortho_to_phon[:n_top//2]):\n",
    "        print(f\"  {i+1:2d}. '{ortho}' → /{phon}/ ({score:.3f})\")\n",
    "    \n",
    "    print(f\"\\nTop {n_top//2} Phoneme → Orthography:\")\n",
    "    for i, (phon, ortho, score) in enumerate(phon_to_ortho[:n_top//2]):\n",
    "        print(f\"  {i+1:2d}. /{phon}/ → '{ortho}' ({score:.3f})\")\n",
    "    \n",
    "    return ortho_to_phon[:n_top], phon_to_ortho[:n_top]\n",
    "\n",
    "# Analyze correspondences\n",
    "ortho_phon_mle, phon_ortho_mle = analyze_grapheme_phoneme(cmu_mle, \n",
    "                                                         'MLE: Grapheme-Phoneme Correspondences')\n",
    "\n",
    "# Compare smoothing effects\n",
    "print(\"\\n=== Smoothing Effect Comparison ===\")\n",
    "sample_pairs = list(cmu_mle.keys())[:10]\n",
    "print(\"Pair\\t\\t\\tMLE (Raw)\\t\\tMLE (Smoothed)\")\n",
    "print(\"=\"*60)\n",
    "for pair in sample_pairs:\n",
    "    raw_xy, raw_yx = cmu_mle[pair]\n",
    "    smooth_xy, smooth_yx = cmu_mle_smooth[pair]\n",
    "    print(f\"{str(pair):20s}\\t{raw_xy:.3f},{raw_yx:.3f}\\t\\t{smooth_xy:.3f},{smooth_yx:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize orthography-phoneme relationships\n",
    "def plot_correspondence_strength(ortho_phon_pairs, title):\n",
    "    \"\"\"Plot correspondence strength visualization.\"\"\"\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    letters = [pair[0] for pair in ortho_phon_pairs[:20]]\n",
    "    phonemes = [pair[1] for pair in ortho_phon_pairs[:20]]\n",
    "    scores = [pair[2] for pair in ortho_phon_pairs[:20]]\n",
    "    \n",
    "    # Create correspondence plot\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Bar plot of correspondence strengths\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.bar(range(len(scores)), scores, alpha=0.7)\n",
    "    plt.xlabel('Orthography-Phoneme Pairs')\n",
    "    plt.ylabel('Association Strength')\n",
    "    plt.title(f'{title}\\nTop 20 Correspondences')\n",
    "    \n",
    "    # Color bars by strength\n",
    "    colors = plt.cm.RdYlBu_r([score/max(scores) for score in scores])\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    # Add correspondence labels\n",
    "    for i, (letter, phoneme) in enumerate(zip(letters, phonemes)):\n",
    "        plt.text(i, scores[i] + 0.01, f\"{letter}→{phoneme}\", \n",
    "                rotation=90, ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: correspondence strength vs frequency\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Calculate approximate frequencies (dummy data for illustration)\n",
    "    freqs = [len([p for p in ortho_phon_pairs if p[0] == letter]) for letter in letters]\n",
    "    \n",
    "    plt.scatter(freqs, scores, alpha=0.7, s=100, c=scores, cmap='RdYlBu_r')\n",
    "    plt.xlabel('Correspondence Frequency')\n",
    "    plt.ylabel('Association Strength')\n",
    "    plt.title('Strength vs Frequency')\n",
    "    \n",
    "    # Add labels for interesting points\n",
    "    for i, (letter, phoneme, score) in enumerate(ortho_phon_pairs[:10]):\n",
    "        if i < len(freqs):\n",
    "            plt.annotate(f'{letter}→{phoneme}', \n",
    "                        (freqs[i], score), \n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.colorbar(label='Association Strength')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot correspondences\n",
    "plot_correspondence_strength(ortho_phon_mle, 'Orthography → Phoneme Correspondences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linguistic Insights\n",
    "\n",
    "The CMU dictionary analysis reveals:\n",
    "\n",
    "1. **Regular Correspondences**: Letters like 'b' → /b/ show high predictability\n",
    "2. **Irregular Mappings**: Letters like 'c' map to multiple phonemes (/k/, /s/)\n",
    "3. **Asymmetric Prediction**: Orthography often predicts phonetics better than vice versa\n",
    "4. **Smoothing Effects**: Laplace smoothing provides more robust estimates for rare correspondences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Analysis: Mushroom Edibility\n",
    "\n",
    "The mushroom dataset demonstrates **classification analysis** where we examine how morphological features predict edibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mushroom data\n",
    "mushroom_data = asymcat.read_sequences(\"../resources/mushrooms.tsv\")\n",
    "mushroom_cooccs = asymcat.collect_cooccs(mushroom_data)\n",
    "\n",
    "print(f\"Mushroom Dataset: {len(mushroom_data)} samples\")\n",
    "print(f\"Feature-class co-occurrences: {len(mushroom_cooccs)}\")\n",
    "\n",
    "# Show sample data structure\n",
    "print(\"\\nSample data:\")\n",
    "for i in range(5):\n",
    "    features, class_label = mushroom_data[i]\n",
    "    print(f\"Features: {features[:3]}..., Class: {class_label}\")\n",
    "\n",
    "# Create scorer\n",
    "mushroom_scorer = asymcat.scorer.CatScorer(mushroom_cooccs)\n",
    "\n",
    "# Compute multiple measures for classification analysis\n",
    "mushroom_measures = {\n",
    "    'MLE': mushroom_scorer.mle(),\n",
    "    'Chi2': mushroom_scorer.chi2(),\n",
    "    'Cramers_V': mushroom_scorer.cramers_v(),\n",
    "    'Theil_U': mushroom_scorer.theil_u(),\n",
    "    'Tresoldi': mushroom_scorer.tresoldi()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictive features for edibility\n",
    "def analyze_classification_features(scores, class_labels=['edible', 'poisonous']):\n",
    "    \"\"\"Analyze which features best predict class labels.\"\"\"\n",
    "    \n",
    "    feature_predictions = {}\n",
    "    \n",
    "    for (feature, class_label), (feat_to_class, class_to_feat) in scores.items():\n",
    "        if class_label in class_labels:\n",
    "            if feature not in feature_predictions:\n",
    "                feature_predictions[feature] = {}\n",
    "            feature_predictions[feature][class_label] = {\n",
    "                'feature_to_class': feat_to_class,\n",
    "                'class_to_feature': class_to_feat\n",
    "            }\n",
    "    \n",
    "    return feature_predictions\n",
    "\n",
    "# Analyze predictive power of features\n",
    "mle_features = analyze_classification_features(mushroom_measures['MLE'])\n",
    "\n",
    "# Find best predictive features\n",
    "print(\"\\n=== Feature Predictive Power for Mushroom Edibility ===\")\n",
    "print(\"\\nFeatures that strongly predict EDIBLE:\")\n",
    "edible_predictors = []\n",
    "for feature, classes in mle_features.items():\n",
    "    if 'edible' in classes:\n",
    "        pred_strength = classes['edible']['feature_to_class']\n",
    "        edible_predictors.append((feature, pred_strength))\n",
    "\n",
    "edible_predictors.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, strength) in enumerate(edible_predictors[:10]):\n",
    "    print(f\"  {i+1:2d}. {feature}: P(edible|{feature}) = {strength:.3f}\")\n",
    "\n",
    "print(\"\\nFeatures that strongly predict POISONOUS:\")\n",
    "poison_predictors = []\n",
    "for feature, classes in mle_features.items():\n",
    "    if 'poisonous' in classes:\n",
    "        pred_strength = classes['poisonous']['feature_to_class']\n",
    "        poison_predictors.append((feature, pred_strength))\n",
    "\n",
    "poison_predictors.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (feature, strength) in enumerate(poison_predictors[:10]):\n",
    "    print(f\"  {i+1:2d}. {feature}: P(poisonous|{feature}) = {strength:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Feature importance for edibility prediction\n",
    "top_edible = edible_predictors[:15]\n",
    "features_e = [f[0] for f in top_edible]\n",
    "strengths_e = [f[1] for f in top_edible]\n",
    "\n",
    "axes[0,0].barh(features_e, strengths_e, alpha=0.7, color='green')\n",
    "axes[0,0].set_xlabel('P(edible | feature)')\n",
    "axes[0,0].set_title('Features Predicting EDIBLE Mushrooms', fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature importance for toxicity prediction\n",
    "top_poison = poison_predictors[:15]\n",
    "features_p = [f[0] for f in top_poison]\n",
    "strengths_p = [f[1] for f in top_poison]\n",
    "\n",
    "axes[0,1].barh(features_p, strengths_p, alpha=0.7, color='red')\n",
    "axes[0,1].set_xlabel('P(poisonous | feature)')\n",
    "axes[0,1].set_title('Features Predicting POISONOUS Mushrooms', fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Comparison of multiple measures for top features\n",
    "top_features_both = set([f[0] for f in top_edible[:8]] + [f[0] for f in top_poison[:8]])\n",
    "measure_comparison = {}\n",
    "\n",
    "for measure_name, scores in mushroom_measures.items():\n",
    "    if measure_name in ['MLE', 'Chi2', 'Cramers_V']:\n",
    "        measure_scores = []\n",
    "        for feature in list(top_features_both)[:10]:\n",
    "            # Find max score for this feature across all classes\n",
    "            max_score = 0\n",
    "            for pair, (xy, yx) in scores.items():\n",
    "                if pair[0] == feature:\n",
    "                    max_score = max(max_score, xy, yx)\n",
    "            measure_scores.append(max_score)\n",
    "        measure_comparison[measure_name] = measure_scores\n",
    "\n",
    "# Plot measure comparison\n",
    "x = np.arange(len(list(top_features_both)[:10]))\n",
    "width = 0.25\n",
    "feature_names = list(top_features_both)[:10]\n",
    "\n",
    "for i, (measure, scores) in enumerate(measure_comparison.items()):\n",
    "    axes[1,0].bar(x + i*width, scores, width, label=measure, alpha=0.8)\n",
    "\n",
    "axes[1,0].set_xlabel('Features')\n",
    "axes[1,0].set_ylabel('Association Strength')\n",
    "axes[1,0].set_title('Comparison of Association Measures', fontweight='bold')\n",
    "axes[1,0].set_xticks(x + width)\n",
    "axes[1,0].set_xticklabels(feature_names, rotation=45, ha='right')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Asymmetric analysis: Feature→Class vs Class→Feature\n",
    "asymmetric_data = []\n",
    "for feature in list(top_features_both)[:8]:\n",
    "    for class_label in ['edible', 'poisonous']:\n",
    "        pair = (feature, class_label)\n",
    "        if pair in mushroom_measures['MLE']:\n",
    "            feat_to_class, class_to_feat = mushroom_measures['MLE'][pair]\n",
    "            asymmetric_data.append({\n",
    "                'feature': feature,\n",
    "                'class': class_label,\n",
    "                'feature_to_class': feat_to_class,\n",
    "                'class_to_feature': class_to_feat\n",
    "            })\n",
    "\n",
    "if asymmetric_data:\n",
    "    df_asym = pd.DataFrame(asymmetric_data)\n",
    "    \n",
    "    # Scatter plot of asymmetric relationships\n",
    "    colors = {'edible': 'green', 'poisonous': 'red'}\n",
    "    for class_label in ['edible', 'poisonous']:\n",
    "        subset = df_asym[df_asym['class'] == class_label]\n",
    "        axes[1,1].scatter(subset['feature_to_class'], subset['class_to_feature'], \n",
    "                         c=colors[class_label], label=class_label, alpha=0.7, s=80)\n",
    "    \n",
    "    # Add diagonal line for symmetry reference\n",
    "    max_val = max(df_asym['feature_to_class'].max(), df_asym['class_to_feature'].max())\n",
    "    axes[1,1].plot([0, max_val], [0, max_val], 'k--', alpha=0.5, label='Symmetric')\n",
    "    \n",
    "    axes[1,1].set_xlabel('P(class | feature)')\n",
    "    axes[1,1].set_ylabel('P(feature | class)')\n",
    "    axes[1,1].set_title('Asymmetric Relationships\\nFeature ↔ Class', fontweight='bold')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Insights\n",
    "\n",
    "The mushroom analysis demonstrates:\n",
    "\n",
    "1. **Strong Predictors**: Certain features are highly diagnostic of edibility/toxicity\n",
    "2. **Asymmetric Prediction**: Features predict class better than class predicts features\n",
    "3. **Multiple Measures**: Different association measures highlight different aspects\n",
    "4. **Safety Implications**: High-confidence predictors are crucial for safety"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Smoothing Effects\n",
    "\n",
    "Demonstrate the mathematical and practical effects of different smoothing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with sparse co-occurrences to demonstrate smoothing\n",
    "sparse_data = [\n",
    "    ('common', 'frequent'), ('common', 'frequent'), ('common', 'frequent'),\n",
    "    ('common', 'rare'), \n",
    "    ('rare_feature', 'frequent'),\n",
    "    ('rare_feature', 'very_rare'),\n",
    "]\n",
    "\n",
    "sparse_cooccs = asymcat.collect_cooccs([sparse_data])\n",
    "\n",
    "# Create scorers with different smoothing methods\n",
    "scorers = {\n",
    "    'MLE (No Smoothing)': asymcat.scorer.CatScorer(sparse_cooccs, smoothing_method='mle'),\n",
    "    'Laplace Smoothing': asymcat.scorer.CatScorer(sparse_cooccs, smoothing_method='laplace'),\n",
    "    'ELE (α=0.1)': asymcat.scorer.CatScorer(sparse_cooccs, smoothing_method='ele', smoothing_alpha=0.1),\n",
    "    'ELE (α=1.0)': asymcat.scorer.CatScorer(sparse_cooccs, smoothing_method='ele', smoothing_alpha=1.0),\n",
    "}\n",
    "\n",
    "# Compute MLE scores for each method\n",
    "smoothing_results = {}\n",
    "for method, scorer in scorers.items():\n",
    "    smoothing_results[method] = scorer.mle()\n",
    "\n",
    "print(\"=== Smoothing Effects Analysis ===\")\n",
    "print(\"Sparse dataset with rare co-occurrences:\")\n",
    "for i, (x, y) in enumerate(sparse_data):\n",
    "    print(f\"  {i+1}. ({x}, {y})\")\n",
    "\n",
    "print(\"\\nSmoothing Effects Comparison:\")\n",
    "print(\"Pair\\t\\t\\t\\tMLE\\t\\tLaplace\\t\\tELE(0.1)\\tELE(1.0)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_pairs = list(smoothing_results['MLE (No Smoothing)'].keys())\n",
    "for pair in all_pairs[:8]:  # Show first 8 pairs\n",
    "    row = f\"{str(pair):30s}\"\n",
    "    for method in scorers.keys():\n",
    "        xy, yx = smoothing_results[method][pair]\n",
    "        row += f\"\\t{xy:.3f}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize smoothing effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Zero vs Non-zero co-occurrences\n",
    "methods = list(smoothing_results.keys())\n",
    "zero_counts = []\n",
    "nonzero_means = []\n",
    "\n",
    "for method in methods:\n",
    "    scores = [max(xy, yx) for xy, yx in smoothing_results[method].values()]\n",
    "    zero_count = sum(1 for s in scores if s == 0.0)\n",
    "    nonzero_scores = [s for s in scores if s > 0.0]\n",
    "    \n",
    "    zero_counts.append(zero_count)\n",
    "    nonzero_means.append(np.mean(nonzero_scores) if nonzero_scores else 0)\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "axes[0,0].bar(x, zero_counts, alpha=0.7, color='red', label='Zero scores')\n",
    "axes[0,0].set_xlabel('Smoothing Method')\n",
    "axes[0,0].set_ylabel('Number of Zero Scores')\n",
    "axes[0,0].set_title('Effect of Smoothing on Zero Scores', fontweight='bold')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(methods, rotation=45, ha='right')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Score distributions\n",
    "for i, method in enumerate(methods):\n",
    "    scores = [max(xy, yx) for xy, yx in smoothing_results[method].values()]\n",
    "    axes[0,1].hist(scores, alpha=0.5, bins=20, label=method.split('(')[0], density=True)\n",
    "\n",
    "axes[0,1].set_xlabel('Maximum Score Value')\n",
    "axes[0,1].set_ylabel('Density')\n",
    "axes[0,1].set_title('Score Distributions by Smoothing Method', fontweight='bold')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Rare vs Common pair treatment\n",
    "# Identify rare and common pairs\n",
    "pair_frequencies = {}\n",
    "for x, y in sparse_data:\n",
    "    pair = (x, y)\n",
    "    pair_frequencies[pair] = pair_frequencies.get(pair, 0) + 1\n",
    "\n",
    "common_pairs = [p for p, freq in pair_frequencies.items() if freq > 1]\n",
    "rare_pairs = [p for p, freq in pair_frequencies.items() if freq == 1]\n",
    "\n",
    "common_scores = {method: [] for method in methods}\n",
    "rare_scores = {method: [] for method in methods}\n",
    "\n",
    "for method in methods:\n",
    "    for pair in common_pairs:\n",
    "        if pair in smoothing_results[method]:\n",
    "            xy, yx = smoothing_results[method][pair]\n",
    "            common_scores[method].append(max(xy, yx))\n",
    "    \n",
    "    for pair in rare_pairs:\n",
    "        if pair in smoothing_results[method]:\n",
    "            xy, yx = smoothing_results[method][pair]\n",
    "            rare_scores[method].append(max(xy, yx))\n",
    "\n",
    "# Plot comparison\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "common_means = [np.mean(common_scores[method]) if common_scores[method] else 0 for method in methods]\n",
    "rare_means = [np.mean(rare_scores[method]) if rare_scores[method] else 0 for method in methods]\n",
    "\n",
    "axes[1,0].bar(x - width/2, common_means, width, label='Common pairs', alpha=0.8)\n",
    "axes[1,0].bar(x + width/2, rare_means, width, label='Rare pairs', alpha=0.8)\n",
    "axes[1,0].set_xlabel('Smoothing Method')\n",
    "axes[1,0].set_ylabel('Mean Score')\n",
    "axes[1,0].set_title('Common vs Rare Pair Treatment', fontweight='bold')\n",
    "axes[1,0].set_xticks(x)\n",
    "axes[1,0].set_xticklabels([m.split('(')[0] for m in methods], rotation=45, ha='right')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Smoothing parameter effects (ELE)\n",
    "alpha_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "alpha_effects = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    ele_scorer = asymcat.scorer.CatScorer(sparse_cooccs, smoothing_method='ele', smoothing_alpha=alpha)\n",
    "    ele_scores = ele_scorer.mle()\n",
    "    \n",
    "    # Calculate mean score for rare pairs\n",
    "    rare_pair_scores = []\n",
    "    for pair in rare_pairs:\n",
    "        if pair in ele_scores:\n",
    "            xy, yx = ele_scores[pair]\n",
    "            rare_pair_scores.append(max(xy, yx))\n",
    "    \n",
    "    alpha_effects.append(np.mean(rare_pair_scores) if rare_pair_scores else 0)\n",
    "\n",
    "axes[1,1].plot(alpha_values, alpha_effects, 'o-', linewidth=2, markersize=8)\n",
    "axes[1,1].set_xlabel('ELE Smoothing Parameter (α)')\n",
    "axes[1,1].set_ylabel('Mean Score for Rare Pairs')\n",
    "axes[1,1].set_title('ELE Smoothing Parameter Effects', fontweight='bold')\n",
    "axes[1,1].set_xscale('log')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Guidelines\n",
    "\n",
    "### When to Use Each Measure\n",
    "\n",
    "1. **MLE (Maximum Likelihood Estimation)**\n",
    "   - **Use for**: Direct conditional probability estimation\n",
    "   - **Interpretation**: P(Y|X) = probability of Y given X\n",
    "   - **Range**: [0, 1]\n",
    "   - **Asymmetric**: Yes, P(Y|X) ≠ P(X|Y)\n",
    "\n",
    "2. **PMI (Pointwise Mutual Information)**\n",
    "   - **Use for**: Information-theoretic association\n",
    "   - **Interpretation**: How much more likely joint occurrence vs independence\n",
    "   - **Range**: (-∞, +∞), 0 = independence\n",
    "   - **Symmetric**: Yes, PMI(X,Y) = PMI(Y,X)\n",
    "\n",
    "3. **Theil's U (Uncertainty Coefficient)**\n",
    "   - **Use for**: Uncertainty reduction measurement\n",
    "   - **Interpretation**: Proportion of uncertainty in Y reduced by knowing X\n",
    "   - **Range**: [0, 1], 1 = perfect prediction\n",
    "   - **Asymmetric**: Yes, U(Y|X) ≠ U(X|Y)\n",
    "\n",
    "4. **Chi-square (χ²)**\n",
    "   - **Use for**: Independence testing\n",
    "   - **Interpretation**: Deviation from expected under independence\n",
    "   - **Range**: [0, +∞), 0 = independence\n",
    "   - **Symmetric**: Yes\n",
    "\n",
    "5. **Jaccard Index**\n",
    "   - **Use for**: Context overlap similarity\n",
    "   - **Interpretation**: Proportion of shared contexts\n",
    "   - **Range**: [0, 1], 1 = identical contexts\n",
    "   - **Symmetric**: Yes\n",
    "\n",
    "### Smoothing Guidelines\n",
    "\n",
    "- **No smoothing (MLE)**: Use with abundant data, when zero probabilities are acceptable\n",
    "- **Laplace smoothing**: General-purpose, adds pseudo-count of 1 to all events\n",
    "- **ELE smoothing**: More sophisticated, uses prior knowledge about event probabilities\n",
    "- **Parameter tuning**: Lower α values (0.1-0.5) for conservative smoothing, higher values (1.0+) for aggressive smoothing\n",
    "\n",
    "### Practical Recommendations\n",
    "\n",
    "1. **Start with MLE** for interpretability\n",
    "2. **Add smoothing** if you have sparse data or zero co-occurrences\n",
    "3. **Compare multiple measures** to get different perspectives\n",
    "4. **Visualize results** to understand patterns\n",
    "5. **Consider domain knowledge** when interpreting asymmetric relationships"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
